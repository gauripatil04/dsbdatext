<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>

<body>
    <h4>OPERATIONS USIG PY ON FACEBOOK DATA</h4>
    <pre>import pandas as pd

df = pd.read_csv (r'C:\Users\Purva\Downloads\facebookdata.csv',sep=";") 

df.describe

df.shape

sub1 = df[['Page total likes', 'Type', 'Category', 'Post Month', 'Post Weekday']].loc[0:15]

sub2 = df[['Lifetime Post reach by people who like your Page', 'Lifetime People who have liked your Page and engaged with your post']].loc[16:30] 

sub1.shape

sub2.shape

sub3=pd.concat([sub1,sub2]) #concatinate two subsets

sub3.shape 

sub4=sub3.sort_values(by='Type',ascending=True) #sort type column in ascending 

sub4

sub4.shape

sub5=sub4.transpose() #transpose row to column

sub5.shape

sub6=pd.pivot_table(sub4,index='Type') #optimization of data,increase the performace

sub6

sub6.shape</pre>
    <br>
    <br>
    <br>
    <hr>
    <br>
    <h4>HEART DISEASE & AIR QUALITY </h4>
    <pre>
    import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

air = pd.read_csv(r"C:\Users\Purva\Downloads\AirQuality.csv",sep=';')
heart = pd.read_csv(r"C:\Users\Purva\Downloads\heart.csv")

air.head()

heart.head()

air.info()

#Dropping CO(GT) and Unnamed columns
air.drop(['CO(GT)','Unnamed: 15','Unnamed: 16'],axis = 1,inplace = True)

air.sample(5)

air.shape

air.isnull().sum()

air.dropna(inplace=True)

air.shape

#Formatting some object columns from strings to floats

air.replace(to_replace=',',value='.',regex=True,inplace=True) 
cols=['C6H6(GT)', 'T', 'RH', 'AH']
for i in cols:
    air[i] = pd.to_numeric(air[i],errors='coerce')

heart.shape

heart.duplicated().sum()

heart.drop_duplicates(inplace=True)
heart.shape

#Replacing -200 to NaN 
air.replace(to_replace=-200,value=np.nan,inplace=True)

air.sample(5)

air.isnull().sum()

NMHC_ratio = air['NMHC(GT)'].isna().sum()/len(air['NMHC(GT)'])
print('The NMHC(GT) sensor has {:.2f}% of missing data.'.format(NMHC_ratio*100))
#Removing NMHC(GT) sensor due to amount of null values
air.drop('NMHC(GT)', axis=1, inplace=True) 

#Formatting Date and Time to datetime type
air['Date'] = pd.to_datetime(air['Date'],dayfirst=True) 
air['Time'] = pd.to_datetime(air['Time'],format= '%H.%M.%S' ).dt.time

air.head()

new_sex=pd.get_dummies(data=heart['sex'],prefix='sex')
new_sex.head()

new_cp=pd.get_dummies(heart['cp'],prefix='chestPain')
new_cp.head()

new_exang=pd.get_dummies(heart['exang'],prefix='exang')
new_exang.head()

new_slope=pd.get_dummies(heart['slope'],prefix='slope')
new_slope.head()

new_thal=pd.get_dummies(heart['thal'],prefix='thal')
new_thal.head()

new_ca=pd.get_dummies(heart['ca'],prefix='ca')
new_ca.head()

# Merge Dataframes
heart_new=pd.concat([heart,new_sex,new_cp,new_ca,new_thal,new_exang,new_slope],axis=1)

heart_new.drop(['sex','cp','thal','exang','ca','slope'],axis=1,inplace=True)
heart_new.head()

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

X=heart_new.drop('target',axis=1)
Y=heart_new['target']

X_scaled = sc.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.2, random_state = 42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#Logistic Regression
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train,y_train)
print("Logistic Regression train score : {:.4f}".format(logreg.score(X_train, y_train)))
print("Logistic Regression test score : {:.4f}".format(logreg.score(X_test, y_test)))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(max_depth=8, random_state=0)
tree.fit(X_train, y_train)
print("Decision Tree train score : {:.4f}".format(tree.score(X_train, y_train)))
print("Decision Tree test score : {:.4f}".format(tree.score(X_test, y_test)))

pred_logreg = logreg.predict(X_test)
pred_tree = tree.predict(X_test)

from sklearn.metrics import confusion_matrix
cm_logreg = confusion_matrix(y_test, pred_logreg)
cm_tree = confusion_matrix(y_test, pred_tree)

plt.subplot(1,5,2)
plt.title("Logistic Regression")
sns.heatmap(cm_logreg, annot=True, cmap="Reds", fmt="d", cbar=False, annot_kws={"size": 12})

plt.subplot(1,5,5)
plt.title("Decision Tree")
sns.heatmap(cm_tree, annot=True, cmap="Reds", fmt="d", cbar=False, annot_kws={"size": 12})
plt.show()

air.isna().sum()

air.drop(columns=['NO2(GT)','NOx(GT)'],inplace=True)

air.dropna(inplace=True)

#Creating a Regression Model of the PT08.S1 sensor:

Y = air['PT08.S1(CO)'] 
X = air.drop(['PT08.S1(CO)','Date', 'Time'], axis=1)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)

from sklearn.ensemble import RandomForestRegressor
model_randomforest = RandomForestRegressor(n_estimators=100)
model_randomforest.fit(X_train, Y_train)

#Evaluating the results with the R² metric
#Test data evaluation
from sklearn import metrics
pred_randomforest = model_randomforest.predict(X_test) #predicted CO concentrations
print('Random Forest Regression Model: R²={:.2f}'.format(metrics.r2_score(Y_test, pred_randomforest)))


</pre>
    <br><br>
    <hr><br><br>
    <h4>INTEGRATE PY & HADOOP</h4>
    <pre>cd Documents/
touch word_count.txt
nano word_count.txt
cat word_count.txt
touch mapper.py
nano mapper.py
cat mapper.py
cat word_count.txt | python3 mapper.py
touch reducer.py
nano reducer.py
cat word_count.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py
start-dfs.sh
start-yarn.sh
hdfs dfs -mkdir /word_count_in_python
hdfs dfs -copyFromLocal /home/purva/Documents/word_count.txt /word_count_in_python
hdfs dfs -ls /
hdfs dfs -ls /word_count_in_python
chmod 777 mapper.py reducer.py
hadoop jar /home/purva/Documents/hadoop-streaming-3.3.4.jar -input /word_count_in_python/word_count.txt -output /word_count_in_python/output2 -mapper "python3 /home/purva/Documents/mapper.py" -reducer " python3 /home/purva/Documents/reducer.py"
hdfs dfs -cat /word_count_in_python/output2/part-00000

</pre>
    <br><br>
    <hr><br><br>
    <h4>MAPREDUCE 2</h4>
    <pre>nano SalesCountryDriver.java
nano SalesCountryReducer.java
nano SalesMapper.java
mkdir lilavati
copy log_file.txt salescountrydriver.java salescountryreducer.java salesmap (download the files first and copy into lilavati)
(copy all the code from all three files and log.txt file too to the directory lilalvati)
cd lilavati
mkdir programs2
mv log_file.txt programs2/
javac -cp `hadoop classpath` -d programs2 SalesCountryDriver.java SalesCountryReducer.java SalesMapper.java 
cd programs2/
ls
jar -cvf SalesCountry.jar -c programs2/ .
hdfs dfs -mkdir testinput5
hadoop fs -put log_file.txt testinput5
hadoop fs -ls testinput5
hadoop jar SalesCountry.jar SalesCountry.SalesCountryDriver testinput5 testoutput5
hadoop fs -cat testoutput5/part-00000
history

</pre>
    <br><br>
    <hr><br><br>
    <h4>MATPLOTLIB & SEABORN</h4>
    <pre>
    import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd 
import seaborn as sns


diabetes=pd.read_csv("Desktop/diabetes.csv")


diabetes 


_axes = plt.subplots(1, 2, sharey=True, figsize=(10, 4))
sns.boxplot(x=’Outcome’, y=’Bloodpressure’, data=diabetes,ax=axes[0])
sns.violinplot(x=’Outcome’ y=’Bloodpressure’, data=diabetes,ax=axes[1])


sns.set(rc={‘figure.figsize’: (16,5)})


sns.boxplot(data=diabetes.select_dtypes(include=’number’))


plt.scatter(diabetes[‘DiabetesPedigreeFunction’], diabetes[‘BMI’])


from mpl_toolkits.mplot3d import Axes3D


x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
y = [5, 6, 2, 3, 13, 4, 1, 2, 4, 8]
z = [2, 3, 3, 3, 5, 7, 9, 11, 9, 10]


sns.set(rc={‘figure.figsize’: (8, 5)})
fig = plt.figure()
ax = fig.add_subplot(111, projection=’3d’)
ax.scatter(x, y, z, c=’r’, marker=’o’)


ax.set_xlabel(‘X Label’), ax.set_ylabel(‘Y Label’), ax.set_zlabel(‘Z Label’)


plt.show()


features = [‘BloodPressure’ ‘SkinThickness’]
diabetes[features].hist(figsize=(10, 4))
 
cars = [‘AUDI’, ‘BMW’, ‘FORD’, ‘TESLA’, ‘JAGUAR’, ‘MERCEDES’]
data = [23, 17, 35, 29, 12, 41]


fig = plt.figure(figsize=(10, 7))
plt.pie(data, labels=cars)
plt.show()


explode = (0.1, 0.0, 0.2, 0.3, 0.0, 0.0)


colors = (“orange”, “cyan”, “brown”, “grey”, “indigo”, “beige”)


wp = {‘linewidth’: 1, ‘edgecolor’: “green”
def func(pct, allvalues):
        absolute = int(pct / 100.*np.sum(allvalues))
        return “{:.1f}%\n({:d} g)”.format(pct, absolute)


fig, ax = plt.subplots(figsize=(10, 7))
Wedges, texts, autotexts = ax.pie(data, autopct=lambda pct:
func(pct, data), explode=explode, labels=cars,
        shadow=True, colors=colors, startangle=90,
wedgeprops=wp,
        textprops=dict(color=”magenta”))
plt.show()
</pre>
    <br><br>
    <hr> <br><br>
    <h4>WEB SCRAPING</h4>
    <pre>import requests
import bs4 


var1=requests.get('https://www.flipkart.com/boat-rockerz-255-pro-258-asap-charge-upto-40-hours-playback-bluetooth-headset/p/itm9d3a2c5e5356a?pid=ACCFZ95M5JTZQH3F&lid=LSTACCFZ95M5JTZQH3FAEVZMP&marketplace=FLIPKART&q=rockerz+255+pro&store=0pm
var1


var1.content


var2=bs4.BeautifulSoup(var1.text)


overall=var2.find(‘div’,{‘class’:’_2d4LTz’}).get_text()
overall


o_rating=var2.find(‘div’,{‘class’:’_2d4LTz’}).get_text()
o_rating


i_rating=var2.find_all(‘div’,{‘class’:’_3LWZlk _1BLPMq’})
for irating in i_rating:
print(irating.get_text()+’\n’)


r=var2.find_all(‘div’,{‘class’:’t-ZTKy’})
for rv in r:
        print(rv.get_text()+’\n’)


reviews=var2.find_all(‘div’,{‘class’:’t-ZTKy’})
for all_reviews in reviews:
        print(all_reviews.get_text()+’\n’)


cname=var2.find_all(‘p’,{‘class’:’_2sc7ZR _2V5EHH’})
for names in cname:
        print(names.get_text()+’\n’)


cust_names=var2.find_all(‘p’,{‘class’:’_2sc7ZR _2V5EHH’})
for c_names in cust_names:
        print(c_names.get_text()+’\n’)</pre>
    <br><br>
    <hr> <br><br>
    <h4>FLIGHT DELAY</h4>
    <pre>1)

hive

set hive.cli.print.header=true;

show databases;

create database flightDelays;

use flightDelays;

create table DelayedFlights(year int, month int, day int, delay int)
row format delimited
fields terminated by ',';

desc DelayedFlights;

//place DelayedFligghts.csv in home

load data local inpath 'DelayedFlights.csv' into table DelayedFlights;

select * from DelayedFlights limit 5;

//find average departure delay per day

select year, month, day, avg(delay) as avg_delay from DelayedFlights group by year, month, day;


2)

//load data using hdfs

//ctrl+c to exit hive

hdfs dfs -mkdir /FlightData

hdfs dfs -put DelayedFlights.csv /FlightData

hdfs dfs -ls /FlightData

//enter hive

hive

use flightDelays;

create table HdfsDelayedFlights(year int, month int, day int, delay int)
row format delimited
fields terminated by ',';

desc HdfsDelayedFlights;

load data inpath '/FlightData/DelayedFlights.csv' into table HdfsDelayedFlights;

select * from HdfsDelayedFlights limit 5;

//find average departure delay per day

select year, month, day, avg(delay) as avg_delay from HdfsDelayedFlights group by year, month, day;
</pre>
</body>

</html>